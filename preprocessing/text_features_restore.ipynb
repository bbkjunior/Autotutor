{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "O3NaQJFGo4wc",
    "outputId": "9ef44eb5-d1da-4427-9571-48d1e1f35220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 4.7MB/s \n",
      "\u001b[?25hCollecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_0zSM2jwoiJJ",
    "outputId": "b599da03-30fb-4a39-e961-591da34b1678"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–,»«'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "full_punctuation = punctuation + \"–\" + \",\" + \"»\" + \"«\"\n",
    "full_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMbtcyDnoiJT",
    "outputId": "b6e2869c-bfd1-4526-917f-d7f67e17b8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Before: ['Ай да А.С.', 'Пушкин!', 'Ай да сукин сын!']\n",
      "After: ['Ай да А.С. Пушкин!', 'Ай да сукин сын!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('russian.pickle')\n",
    "text = \"Ай да А.С. Пушкин! Ай да сукин сын!\"\n",
    "print(\"Before:\", nltk.sent_tokenize(text))\n",
    "print(\"After:\", tokenizer.tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "9sqVPhWXoiJa"
   },
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    clean_sentence = []\n",
    "    #print(sentences)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        clean_text= ''\n",
    "        for word in words:\n",
    "            clean_word = ''\n",
    "            for char in word:\n",
    "                if char != \" \" and char not in full_punctuation:\n",
    "                    clean_word += char.lower()\n",
    "            clean_text += clean_word + ' '\n",
    "        clean_text = re.sub(' +', ' ', clean_text)\n",
    "        clean_text = clean_text.strip()\n",
    "        clean_sentence.append(clean_text)\n",
    "    return clean_sentence\n",
    "\"\"\"    \n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens #if token not in russian_stopwords\n",
    "              if token != \" \" and token.strip() not in full_punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\"\"\"\n",
    "    \n",
    "\n",
    "def clean_and_write_file(file, destination):\n",
    "    preprocessed_text = []\n",
    "    lines = 0\n",
    "    with open (file, \"r\", encoding = \"utf-8\") as file:\n",
    "        all_lines = file.readlines()\n",
    "        bar = progressbar.ProgressBar(maxval=len(all_lines),\n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        bar.start()\n",
    "        for line in all_lines:\n",
    "            t = preprocess_text(line)\n",
    "            #print(t)\n",
    "            preprocessed_text.extend(t)\n",
    "            lines += 1\n",
    "            bar.update(lines)\n",
    "            sleep(0.1)               \n",
    "            \n",
    "    with open (destination, \"w+\", encoding = \"utf-8\") as write_file:\n",
    "        for sentence in preprocessed_text:\n",
    "            #print(\"write\", sentence)\n",
    "            write_file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpleFjJFoiJg",
    "outputId": "be6de98e-2da1-4175-c971-a1de9a1a7690"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    }
   ],
   "source": [
    "clean_and_write_file(\"text_8.txt\", \"text_8_processed_tr.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exQg_sqnoiJk"
   },
   "source": [
    "# calculate_lix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "I9gyZWMloiJl"
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def calculate_lix(processed_text):\n",
=======
    "def calculate_lix_from_file(processed_text):\n",
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "      with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "        sentences_count = len(sentences)\n",
    "        words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "        long_words_count = 0 #more than 6\n",
    "        for line in sentences:\n",
    "            for word in line.split():\n",
    "                if len(word) > 6:\n",
    "                    long_words_count += 1\n",
    "        lix = words_count/ sentences_count + (long_words_count * 100) / words_count\n",
    "        print(lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-pu09zAoiJo",
    "outputId": "6b08fc77-d579-44e4-c2aa-4075f1cab41d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.771679197994985\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "calculate_lix(\"text_8_processed.txt\")"
=======
    "calculate_lix_from_file(\"text_8_processed.txt\")"
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "UrtH5AyYoiJt"
   },
   "outputs": [],
   "source": [
    "def lemmatize_line(text,morph):\n",
    "    \"\"\"\n",
    "    составляем словарь лемматизированных существительных и словарь всех лемм \n",
    "    \"\"\"\n",
    "    lemm_split_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        parsed_word = morph.parse(word)[0]\n",
    "        lemma = parsed_word.normal_form      \n",
    "        lemm_split_text.append(lemma)\n",
    "    return ' '.join(lemm_split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lImCWAuIoiJy",
    "outputId": "7b3a502a-b857-47a0-a4c4-0cf25ef92899"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-36370d92bce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmorph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlemmatize_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"составляем словарь лемматизированных существительных\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmorph\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatize_line' is not defined"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmatize_line(\"составляем словарь лемматизированных существительных\",morph )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "Ynj_2HvpoiJ6"
   },
   "outputs": [],
   "source": [
    "def lemmatize_text_from_file(file, destination):\n",
    "    lemm_text_lines = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    with open(file, \"r\", encoding = \"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            lemm_line = lemmatize_line(line, morph)\n",
    "            lemm_text_lines.append(lemm_line)\n",
    "    with open(destination, \"w\", encoding = \"utf-8\") as d:\n",
    "        for line in lemm_text_lines:\n",
    "            #print(\"write\", sentence)\n",
    "            d.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "2rOpp2XUoiJ-"
   },
   "outputs": [],
   "source": [
    "lemmatize_text_from_file(\"text_8_clean.txt\", \"text_8_lemm_n.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLauhnT3oiKE"
   },
   "source": [
    "# type_token_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "KpsDs4iZoiKF"
   },
   "outputs": [],
   "source": [
    "def calculate_type_token_ratio(lemm_text):\n",
    "      with open (lemm_text, \"r\", encoding = \"utf-8\") as file:\n",
    "          sentences = file.readlines()\n",
    "          pos_list = []\n",
    "          words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "          all_words = []\n",
    "          for sentence in sentences:\n",
    "              words = sentence.split()\n",
    "              for word in words:\n",
    "                  all_words.append(word)\n",
    "                  \n",
    "          unqie_words = set(all_words)\n",
    "          types = len(unqie_words)\n",
    "          tokens = len (all_words)\n",
    "          \n",
<<<<<<< HEAD
    "          return types/tokens\n"
=======
    "          return types/tokens\n",
    "    \n"
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5kCEFF_oiKJ",
    "outputId": "eb8cbcf6-03a6-41ad-d56b-ef36975fe637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193798449612403"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_type_token_ratio(\"text_8_lemm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "jy9DzBDroiKN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "29HYmcTSoiKQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "g-CkAJ5UoiKX"
   },
   "source": [
    "# РАЗНООБРАЗИЕ ЛИЦ -- обсудить необходимость фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsuSTOsGoiKZ",
    "outputId": "a1664fb9-cc27-4792-c973-fd6167d38d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'делать', 'wt': 1, 'gr': 'V,несов,пе=прош,мн,изъяв'}], 'text': 'делали'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "print(mystem.analyze(\"делали\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DN5s4z0oiKi",
    "outputId": "3c68eb18-fb02-499b-aeec-552f652015d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='делать', tag=OpencorporaTag('INFN,impf,tran'), normal_form='делать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'делать', 303, 0),))]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"делать\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htviTkNOoiKn"
   },
   "source": [
    "# Отношение нефинитных глаголов, инфинитивов, бы, себя, причастия, деепричастия ко всем глаголам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "GqysBkYGoiKp"
   },
   "outputs": [],
   "source": [
    "# https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "_SYDCS2yoiKu"
   },
   "outputs": [],
   "source": [
    "нефинитных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "voiP2JTXoiKy",
    "outputId": "30ff19e4-7762-4e6e-9714-2b1cb38a5dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='делать', tag=OpencorporaTag('INFN,impf,tran'), normal_form='делать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'делать', 303, 0),))]\n",
      "[Parse(word='сделать', tag=OpencorporaTag('INFN,perf,tran'), normal_form='сделать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сделать', 614, 0),))]\n",
      "[Parse(word='пить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='пить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'пить', 444, 0),))]\n"
     ]
    }
   ],
   "source": [
    "#инфинитивов\n",
    "print(morph.parse(\"делать\"))\n",
    "print(morph.parse(\"сделать\"))\n",
    "print(morph.parse(\"пить\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "DTia1J6eoiK6"
   },
   "outputs": [],
   "source": [
    "#бы просто считаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "sgLJnPV7oiLB"
   },
   "outputs": [],
   "source": [
    "#https://pymorphy2.readthedocs.io/en/latest/_modules/pymorphy2/tagset.html?highlight=impf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpM5xbn5oiLF",
    "outputId": "d70d28e2-9f82-4dfc-ece7-e16938a65da8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#возвратные - себя сь ся\n",
    "word = \"возвращаться\"\n",
    "word.endswith(\"ся\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4GqVkbMoiLI",
    "outputId": "35ae7390-e4fa-4cc6-cd56-bb84fe2de4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='проезжая', tag=OpencorporaTag('GRND,impf,tran pres'), normal_form='проезжать', score=0.666666, methods_stack=((<DictionaryAnalyzer>, 'проезжая', 206, 94),)), Parse(word='проезжая', tag=OpencorporaTag('ADJF femn,sing,nomn'), normal_form='проезжий', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'проезжая', 585, 7),))]\n",
      "[Parse(word='делающий', tag=OpencorporaTag('PRTF,impf,tran,pres,actv masc,sing,nomn'), normal_form='делать', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'делающий', 303, 13),)), Parse(word='делающий', tag=OpencorporaTag('PRTF,impf,tran,pres,actv inan,masc,sing,accs'), normal_form='делать', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'делающий', 303, 17),))]\n",
      "[Parse(word='делав', tag=OpencorporaTag('GRND,impf,tran past'), normal_form='делать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'делав', 303, 122),))]\n"
     ]
    }
   ],
   "source": [
    "#причастия\n",
    "print(morph.parse(\"проезжая\"))\n",
    "print(morph.parse(\"делающий\"))\n",
    "print(morph.parse(\"делав\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "OA_ubf-XoiLM"
   },
   "outputs": [],
   "source": [
    "#'PRTF',  # причастие (полное)\n",
    "#'PRTS',  # причастие (краткое)\n",
    "#'GRND',  # деепричастие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1trsKvkoiLO"
   },
   "source": [
    "# Количество бы, не, ни (проблема отрицания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "82yslDqcoiLP"
   },
   "source": [
    "просто поиск слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "ADSsM2yfoiLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "zyEtF96joiLV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "ZLhxWQ81oiLc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-s4Oa4UoiLg",
    "outputId": "69344c18-4b29-47e9-eb4b-926b40cc2747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в классе был урок русского языка\n",
      "\n",
      "PR=\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "A=(вин\n",
      "A\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "преподаватель читал новый текст\n",
      "\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "A=(вин\n",
      "A\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "студенты внимательно слушали\n",
      "\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "ADV=\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "извините можно войти\n",
      "\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "ADV\n",
      "ADV\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "я опоздал сказал джон\n",
      "\n",
      "SPRO\n",
      "SPRO\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "что случилось джон\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f036ee0bc790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmystemmed_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmystem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmystemmed_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmystemmed_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystemmed_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open (\"text_8_processed.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    \n",
    "    sentences = file.readlines()\n",
    "    words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        mystemmed_line = mystem.analyze(sentence)\n",
    "        for mystemmed_word in mystemmed_line:\n",
    "            keys = list(mystemmed_word.keys())\n",
    "            values = list(mystemmed_word.values())\n",
    "            word  = mystemmed_word['text']\n",
    "            if ('analysis' not in keys):\n",
    "                pass\n",
    "            elif(mystemmed_word['analysis'] ==[]):\n",
    "                pass\n",
    "            else:\n",
    "              grammar = mystemmed_word['analysis'][0]['gr']\n",
    "              grammar_sep_by_comma = grammar.split(',')\n",
    "              print(grammar_sep_by_comma[0])\n",
    "              if (len(grammar_sep_by_comma) == 1):\n",
    "                    \n",
    "                    pass\n",
    "              else:\n",
    "                pos = re.match('[A-Zs]+', grammar_sep_by_comma[0])[0]\n",
    "                #print(pos)\n",
    "           # print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "6LSS7BDNoiLm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
<<<<<<< HEAD
=======
    "collapsed": true,
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
    "id": "nXLVeRwToiLo"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(processed_text, destination):\n",
    "      mystem = Mystem()\n",
    "      with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "          sentences = file.readlines()\n",
    "          clean_sentences = []\n",
    "          for sentence in sentences:\n",
    "              clean_sentence = ''\n",
    "              mystemmed_line = mystem.analyze(sentence)\n",
    "              for mystemmed_word in mystemmed_line:\n",
    "                  keys = list(mystemmed_word.keys())\n",
    "                  values = list(mystemmed_word.values())\n",
    "                  word  = mystemmed_word['text']\n",
    "                  if ('analysis' not in keys):\n",
    "                      pass\n",
    "                  elif(mystemmed_word['analysis'] ==[]):\n",
    "                      pass\n",
    "                  else:\n",
    "                    grammar = mystemmed_word['analysis'][0]['gr']\n",
    "                    grammar_sep_by_comma = grammar.split(',')\n",
    "                    print(grammar_sep_by_comma[0])\n",
    "                    if (len(grammar_sep_by_comma) == 1):\n",
    "                          pass\n",
    "                    else:\n",
    "                          pos = re.match('[A-Zs]+', grammar_sep_by_comma[0])[0]\n",
    "                          print(pos)\n",
    "\n",
    "                    \n",
    "                  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_features_restore.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.4"
=======
   "version": "3.6.2"
>>>>>>> 8388de37598989d552d3353b948a3c0da13af232
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
