{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.59\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "spiList = [5.55, 5.72, 7.3, 7.75, 8.4, 9, 8.8, 8.2]\n",
    "print(statistics.mean(spiList))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "fasttext = FastTextKeyedVectors.load(\"D:/fasttext_word2vec/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"lyashevskaya_freq_dict.json\" , \"r\", encoding=\"utf-8\") as f:\n",
    "    lyashevskaya_freq_dict = json.load(f)\n",
    "    \n",
    "with open (\"C:\\Autotutor\\improved_approach\\colloc\\music_unigr_freq.json\", \"r\", encoding='utf-8') as f:\n",
    "    unigramm_db = json.load(f)\n",
    "    \n",
    "    \n",
    "with open (\"D:\\input\\music_smart_colloc_freq.json\", \"r\", encoding='utf-8') as f:\n",
    "    colloc_db = json.load(f)\n",
    "    \n",
    "words_indexed_dict = []\n",
    "with open(\"big_musiciancolloc_lit.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        words_indexed_dict.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'оказываться'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_indexed_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAME = 'big_musician'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "marked_word_db = pd.read_csv(USER_NAME +  \"_word_db.csv\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(ngramm):\n",
    "    ngramm_lemmas_list = ngramm.split()\n",
    "    freq_list = []\n",
    "    local_corpora_freq_list = []\n",
    "    w2v_list = []\n",
    "    for lemma in ngramm_lemmas_list:\n",
    "        \n",
    "        try:\n",
    "            w2v = fasttext[lemma]\n",
    "            w2v_list.append(w2v)\n",
    "        except:\n",
    "            pass\n",
    "        \"\"\"\n",
    "        print(lemma)\n",
    "        w2v = fasttext[lemma]\n",
    "        w2v_list.append(w2v)\n",
    "        \"\"\"\n",
    "        \n",
    "        if lemma in lyashevskaya_freq_dict:\n",
    "            freq_list.append(lyashevskaya_freq_dict[lemma])\n",
    "        else:\n",
    "            freq_list.append(0)\n",
    "            \n",
    "        if lemma in unigramm_db:\n",
    "            local_corpora_freq_list.append(unigramm_db[lemma])\n",
    "        else:\n",
    "            local_corpora_freq_list.append(0)\n",
    "    colleted_w2v_count = len(w2v_list)\n",
    "    if colleted_w2v_count > 0:\n",
    "        vect_sum = 300 * [0]\n",
    "        for w2v in w2v_list:\n",
    "            vect_sum += w2v\n",
    "        vect_sum /=  colleted_w2v_count\n",
    "    else:\n",
    "        print(ngramm_lemmas_list, \"none in fasttext\")\n",
    "        vect_sum = None\n",
    "    if len(local_corpora_freq_list) > 0:\n",
    "        local_corpora_mean = statistics.mean(local_corpora_freq_list)\n",
    "    else:\n",
    "        local_corpora_mean = None\n",
    "    if len(freq_list) > 0:\n",
    "        freq_mean = statistics.mean(freq_list)\n",
    "    else:\n",
    "        freq_mean = None\n",
    "    \n",
    "    #return (local_corpora_mean,freq_mean, vect_sum)#????\n",
    "    return (freq_mean,local_corpora_mean, vect_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/57842 [00:00<?, ?it/s]\n",
      "  0%|                                                                               | 16/57842 [00:00<49:30, 19.46it/s]\n",
      "  0%|                                                                               | 31/57842 [00:01<52:18, 18.42it/s]\n",
      "  0%|                                                                             | 32/57842 [00:02<1:15:23, 12.78it/s]\n",
      "  0%|                                                                             | 37/57842 [00:03<1:26:22, 11.15it/s]\n",
      "  0%|                                                                             | 38/57842 [00:04<1:44:10,  9.25it/s]\n",
      "  0%|                                                                             | 48/57842 [00:04<1:38:34,  9.77it/s]\n",
      "  0%|                                                                             | 49/57842 [00:05<1:52:00,  8.60it/s]\n",
      "  0%|                                                                             | 54/57842 [00:06<1:56:43,  8.25it/s]\n",
      "  0%|                                                                             | 66/57842 [00:07<1:50:54,  8.68it/s]\n",
      "  0%|                                                                             | 70/57842 [00:08<1:56:00,  8.30it/s]\n",
      "  2%|█▏                                                                          | 924/57842 [07:10<7:22:05,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '0'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                         | 1027/57842 [08:16<7:37:24,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                        | 1817/57842 [17:24<8:56:37,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15', '00'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                        | 1944/57842 [18:46<8:59:48,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17', '00'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                        | 2003/57842 [19:28<9:02:47,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22', '00'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                        | 2021/57842 [19:40<9:03:22,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18', '00'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▌                                                                      | 2768/57842 [30:17<10:02:43,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25', '17'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▌                                                                      | 2781/57842 [30:26<10:02:46,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '20'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▌                                                                      | 2808/57842 [30:50<10:04:22,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['а', 'т'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████▏                                                | 18650/57842 [3:49:02<8:01:19,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'a'] none in fasttext\n",
      "['n', 'w'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████████▊                            | 35175/57842 [6:42:13<4:19:11,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████▍                   | 42119/57842 [7:54:43<2:57:12,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'w', 'a'] none in fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 57842/57842 [10:38:47<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_all_words_similarity = {}\n",
    "with open(\"D:\\input\\music_smart_colloc_freq.json\", 'r', encoding = 'utf-8') as f:\n",
    "    music_colloc_db = json.load(f)\n",
    "    all_colloc = []\n",
    "    all_colloc.extend(list(music_colloc_db[str(2)].keys()))\n",
    "    all_colloc.extend(list(music_colloc_db[str(3)].keys()))\n",
    "    all_colloc.extend(list(music_colloc_db[str(4)].keys()))\n",
    "    for colloc_word_ind in tqdm(range(len(all_colloc))):\n",
    "        if not all_colloc[colloc_word_ind].isdigit():\n",
    "            current_word_similarity = []\n",
    "            colloc_word_vector = get_vector(all_colloc[colloc_word_ind])\n",
    "            if np.any(colloc_word_vector[0]) and np.any(colloc_word_vector[1]) and np.any(colloc_word_vector[2]):\n",
    "                freq_vec = np.array([colloc_word_vector[0], colloc_word_vector[1]]).reshape(1,-1)\n",
    "                colloc_w2v = np.array(colloc_word_vector[2]).reshape(1,-1)\n",
    "                absolute_equality_found = False\n",
    "                for db_word_index in range(len(marked_word_db)):\n",
    "                    compared_raw_word_vect = marked_word_db.iloc[db_word_index][:-1]\n",
    "                    comp_word2vec = compared_raw_word_vect[2:]\n",
    "                    comp_colloc_w2v = np.array(comp_word2vec).reshape(1,-1)\n",
    "                    if np.any(comp_word2vec) and np.any(colloc_w2v):\n",
    "                        w2v_sim = cosine_similarity(colloc_w2v,comp_colloc_w2v)\n",
    "                    else:\n",
    "                        w2v_sim = [[0]]\n",
    "                    comp_freq_vec = np.array([compared_raw_word_vect[1], compared_raw_word_vect[0]]).reshape(1,-1)\n",
    "                    sim_freq = cosine_similarity(freq_vec,comp_freq_vec)\n",
    "                    #print(\"freq\", sim_freq)\n",
    "                    #print(comp_freq_vec)\n",
    "                    w2v_sim = w2v_sim[0][0]\n",
    "                    sim_freq = sim_freq[0][0]\n",
    "\n",
    "                    average_similarity = w2v_sim*0.8 + sim_freq*0.2\n",
    "                    if average_similarity >= 1:\n",
    "                        absolute_equality_found = True\n",
    "\n",
    "                    word_comp_json = {\"marked_colloc_db_index\":db_word_index, \"sim_colloc\":words_indexed_dict[db_word_index],\"w2v_sim\":w2v_sim,\"freqence_similarity\":sim_freq}\n",
    "                    if average_similarity> 0.6 and w2v_sim > 0.65 and absolute_equality_found == False:\n",
    "                        current_word_similarity.append((word_comp_json,average_similarity))\n",
    "                    elif average_similarity >= 0.75 and absolute_equality_found == True:\n",
    "                        current_word_similarity.append((word_comp_json,average_similarity))\n",
    "                current_word_similarity.sort(key = operator.itemgetter(1), reverse= True)\n",
    "                if absolute_equality_found == True:\n",
    "                    only_absolute_equalitie = []\n",
    "                    for simil_el in current_word_similarity:\n",
    "                        if simil_el[1] >= 0.85:\n",
    "                            only_absolute_equalitie.append(simil_el)\n",
    "                    list_of_all_words_similarity[all_colloc[colloc_word_ind]] = only_absolute_equalitie[:5]\n",
    "                else:\n",
    "                    list_of_all_words_similarity[all_colloc[colloc_word_ind]] = current_word_similarity[:5]\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"big_musician_ngr_colloc_similairities.json\", 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(list_of_all_words_similarity, f , indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"big_musician_unigr_colloc_similairities.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, dict)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_of_all_words_similarity),type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list_of_all_words_similarity.copy()\n",
    "d.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"big_musician_overall_colloc_similairities.json\", 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(d, f , indent = 4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
