{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "fasttext = FastTextKeyedVectors.load(\"D:/fasttext_word2vec/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing import get_text_map\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lenta = pd.read_csv(\"./articles/music_lenta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_index = [0,\n",
    "17,\n",
    "26,\n",
    "32,\n",
    "98,\n",
    "121,\n",
    "130,133,\n",
    "200,\n",
    "231,240,316,331,334,336,366,371]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maps_json_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_maps_json_dict = {}\n",
    "for tex_ind in texts_index:\n",
    "    text_map = get_text_map(texts_lenta.iloc[tex_ind]['text'],unigramm_db_path ='./colloc/music_unigr_freq.json' , \n",
    "                            colloc_db_path = \"/Users/nigula/Desktop/long_term_storage/colloc/music_smart_colloc_freq.json\", raw_text_input=True)\n",
    "    text_maps_json_dict[tex_ind] = text_map\n",
    "    \n",
    "#\"/Users/nigula/Desktop/long_term_storage/colloc/music_smart_colloc_freq.json\"\n",
    "#\"D:\\input\\music_smart_colloc_freq.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text_sent_collocind_dict = {\"0\":{'0':[9,11],'2':[26,35],'5':[13,27]},\n",
    "                                     \"17\":{'0':[7,18],'2':[8,19],'4':[2,16],'8':[7,25],'10':[14,24],'12':[9,24]},\n",
    "                                     \"26\":{'0':[6,19],'5':[5,11],'7':[0,6],'13':[13,27]},\n",
    "                                     \"32\":{'3':[9,20],'5':[5,17],'14':[6,17],'18':[3,19]},\n",
    "                                    \"98\":{'0':[11,16],'4':[3,6],'5':[1,12],'8':[7,12]},\n",
    "                                    \"121\":{'1':[2,7],'3':[5,19],'4':[0,15],'4':[0,15],'6':[13,17],'11':[0,5]},#'0':[11,16],\n",
    "                                    \"130\":{'0':[17,28],'1':[0,7],'4':[1,8],'5':[6,10],'8':[15,19],'9':[12,28],'16':[0,12]},#,'3':[2,9]\n",
    "                                    \"133\":{'2':[19,35],'6':[8,13],'8':[3,12],'9':[7,16],'18':[8,19],'20':[0,8],'20':[0,8],'40':[11,20],'49':[0,15]},\n",
    "                                     \"200\":{'0':[11,14],'2':[11,17],'4':[0,15],'10':[0,10],'12':[5,12]},\n",
    "                                      \"231\":{'2':[0,7],'4':[0,19],'7':[0,12],'12':[13,30]},\n",
    "                                    \"240\":{'1':[0,13],'11':[0,8],'15':[5,20]},\n",
    "                                     \"316\":{'0':[12,20],'5':[8,13],'7':[9,20],'11':[20,27]},\n",
    "                                     \"331\":{'2':[14,23],'4':[4,12],'10':[4,14]},\"334\":{'1':[4,10],'2':[0,3],'5':[4,12],'8':[1,9]},\n",
    "                                     \"336\":{'1':[0,10],'7':[0,9],'10':[1,7],'13':[3,5]},\"366\":{'0':[0,4],'1':[0,12],'5':[15,24],'8':[8,20],'10':[0,15],'11':[0,18],'15':[2,11]},\n",
    "                                     \"371\":{'0':[9,21],'2':[13,20],'3':[11,25],'6':[2,7],'12':[0,16],'21':[0,30]}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(question_text_sent_collocind_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig [0, 2, 5]\n",
      "sorted [0, 2, 5]\n",
      "orig [0, 2, 4, 8, 10, 12]\n",
      "sorted [0, 2, 4, 8, 10, 12]\n",
      "orig [0, 5, 7, 13]\n",
      "sorted [0, 5, 7, 13]\n",
      "orig [3, 5, 14, 18]\n",
      "sorted [3, 5, 14, 18]\n",
      "orig [0, 4, 5, 8]\n",
      "sorted [0, 4, 5, 8]\n",
      "orig [0, 1, 3, 4, 6, 11]\n",
      "sorted [0, 1, 3, 4, 6, 11]\n",
      "orig [0, 1, 3, 4, 5, 8, 9, 16]\n",
      "sorted [0, 1, 3, 4, 5, 8, 9, 16]\n",
      "orig [2, 6, 8, 9, 18, 20, 40, 49]\n",
      "sorted [2, 6, 8, 9, 18, 20, 40, 49]\n",
      "orig [2, 4, 7, 12]\n",
      "sorted [2, 4, 7, 12]\n",
      "orig [1, 11, 15]\n",
      "sorted [1, 11, 15]\n",
      "orig [0, 5, 7, 11]\n",
      "sorted [0, 5, 7, 11]\n",
      "orig [2, 4, 10]\n",
      "sorted [2, 4, 10]\n",
      "orig [1, 2, 5, 8]\n",
      "sorted [1, 2, 5, 8]\n",
      "orig [1, 7, 10, 13]\n",
      "sorted [1, 7, 10, 13]\n",
      "orig [0, 1, 5, 8, 10, 11, 15]\n",
      "sorted [0, 1, 5, 8, 10, 11, 15]\n",
      "orig [0, 2, 3, 6, 12, 21]\n",
      "sorted [0, 2, 3, 6, 12, 21]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: OrderedDict([(0, True), (2, True), (5, True)]),\n",
       " 17: OrderedDict([(0, True),\n",
       "              (2, True),\n",
       "              (4, True),\n",
       "              (8, True),\n",
       "              (10, True),\n",
       "              (12, True)]),\n",
       " 26: OrderedDict([(0, True), (5, True), (7, True), (13, True)]),\n",
       " 32: OrderedDict([(3, True), (5, True), (14, True), (18, True)]),\n",
       " 98: OrderedDict([(0, True), (4, True), (5, True), (8, True)]),\n",
       " 121: OrderedDict([(0, True),\n",
       "              (1, True),\n",
       "              (3, True),\n",
       "              (4, True),\n",
       "              (6, True),\n",
       "              (11, True)]),\n",
       " 130: OrderedDict([(0, True),\n",
       "              (1, True),\n",
       "              (3, True),\n",
       "              (4, True),\n",
       "              (5, True),\n",
       "              (8, True),\n",
       "              (9, True),\n",
       "              (16, True)]),\n",
       " 133: OrderedDict([(2, True),\n",
       "              (6, True),\n",
       "              (8, True),\n",
       "              (9, True),\n",
       "              (18, True),\n",
       "              (20, True),\n",
       "              (40, True),\n",
       "              (49, True)]),\n",
       " 231: OrderedDict([(2, True), (4, True), (7, True), (12, True)]),\n",
       " 240: OrderedDict([(1, True), (11, True), (15, True)]),\n",
       " 316: OrderedDict([(0, True), (5, True), (7, True), (11, True)]),\n",
       " 331: OrderedDict([(2, True), (4, True), (10, True)]),\n",
       " 334: OrderedDict([(1, True), (2, True), (5, True), (8, True)]),\n",
       " 336: OrderedDict([(1, True), (7, True), (10, True), (13, True)]),\n",
       " 366: OrderedDict([(0, True),\n",
       "              (1, True),\n",
       "              (5, True),\n",
       "              (8, True),\n",
       "              (10, True),\n",
       "              (11, True),\n",
       "              (15, True)]),\n",
       " 371: OrderedDict([(0, True),\n",
       "              (2, True),\n",
       "              (3, True),\n",
       "              (6, True),\n",
       "              (12, True),\n",
       "              (21, True)])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_dict = {}\n",
    "text_ind_keys = list(question_text_sent_collocind_dict.keys())\n",
    "text_ind_keys = [int(numb) for numb in text_ind_keys]\n",
    "text_ind_keys.sort()\n",
    "for key in text_ind_keys:\n",
    "    answers_dict[key] = OrderedDict()\n",
    "    key_list = list(question_text_sent_collocind_dict[str(key)].keys())\n",
    "    key_list = [int(numb) for numb in key_list]\n",
    "    print(\"orig\", key_list)\n",
    "    key_list.sort()\n",
    "    print(\"sorted\",key_list)\n",
    "    for setn_keys in key_list:\n",
    "        answers_dict[key][setn_keys] = True\n",
    "answers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(ans_dict_enze.keys())), len(list(question_text_sent_collocind_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вокалист американской рок-группы Limp Bizkit Фред Дерст откликнулся на призыв главы Крыма Сергея Аксенова к мировым знаменитостям, изъявив желание обосноваться на полуострове. Об этом сообщают в четверг, 8 октября, «Известия». В письме музыканта в адрес администрации Крыма говорится, что он «был бы счастлив», получив российский паспорт, купить «симпатичный домик в Крыму». Дерст, по его словам, намерен жить на полуострове по полгода, снимая там фильмы и сериалы, которые бы позволили вывести Россию «на высокий уровень в этом бизнесе». Таким образом вокалист Limp Bizkit хочет стать «частью великого будущего Крыма и России». Дерст отмечает, что не может отказаться от гражданства США, поскольку в Лос-Анджелесе живет его сын. Однако он обещает «привлечь в Россию больше американцев». «Я могу помочь американцам понять, как прекрасна Россия. Я буду создавать фильмы, сериалы, музыку, новые бренды в Крыму, но при этом мне необходимо иметь два паспорта — это важно. Я думаю, проблем с этим не будет», — приводят «Известия» цитату из письма. Для реализации своих замыслов Дерст рассчитывает на поддержку правительства. В его планах, помимо прочего, организация кинофестиваля в Крыму и создание киностудии, что позволило бы привлечь к полуострову «невероятное внимание мирового сообщества». В будущем музыкант не исключает, что займется в России политикой, чтобы «помочь многим людям и вдохновить их». Он также надеется на встречу с президентом России Владимиром Путиным. «Я думаю, что я мог бы стать другом президента Путина. Уверен в этом... Я думаю, что президент Путин узнает, что я за человек, посмотрев мне в глаза, и поймет, что у него есть союзник, который поможет во многих вещах. Я уверен, что мы можем сделать много важных дел вместе, и это поможет России и поможет людям по всему миру понять, что Путин — \"отличный парень\" с четкими моральными принципами и хороший человек», — говорится в письме. 1 сентября Фред Дерст заявил о желании получить российское гражданство и приобрести недвижимость в России. В середине сентября глава Крыма Сергей Аксенов пригласил представителей мирового шоу-бизнеса и большого спорта обосноваться на полуострове и таким образом создать «аналог американского Беверли-Хиллс». «Если уважаемые представители шоу-бизнеса, спорта, другие знаменитости захотят обосноваться в Крыму — милости просим. Мы можем предоставить помощь в поиске подходящего места у моря, среди гор, лесов или степей, в любом случае — среди великолепной природы и в чудесном климате Крыма», — отмечалось в обращении главы Крыма. Американский боксер Рой Джонс 20 августа написал официальное прошение о получении российского паспорта, а 12 сентября президент Владимир Путин подписал указ, предоставляющий спортсмену гражданство РФ. 10 сентября французский актер Сами Насери, известный по кинофраншизе «Такси», заявил о готовности оформить российский паспорт, если ему кто-нибудь предложит. В 2013 российский паспорт получил актер Жерар Депардье.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_lenta.iloc[371]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_map = get_text_map(texts_lenta.iloc[371]['text'],unigramm_db_path = \"C:\\Autotutor\\improved_approach\\colloc\\music_unigr_freq.json\",\n",
    "                            colloc_db_path = \"D:\\input\\music_smart_colloc_freq.json\",raw_text_input=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: [(0, 'за'),\n",
       "  (1, '21летний певица'),\n",
       "  (3, 'из'),\n",
       "  (4, 'зеленодольск'),\n",
       "  (5, 'татарстан'),\n",
       "  (6, 'быть'),\n",
       "  (7, 'отдавать'),\n",
       "  (8, 'более'),\n",
       "  (9, 'половина'),\n",
       "  (10, 'голос телезритель')]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tex_map['overall_colloc_text'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_dict_enze ={0: OrderedDict([(0, True), (2, True), (5, True)]),\n",
    " 17: OrderedDict([(0, True),\n",
    "              (2, False),\n",
    "              (4, True),\n",
    "              (8, True),\n",
    "              (10, True),\n",
    "              (12, True)]),\n",
    " 26: OrderedDict([(0, True), (5, True), (7, True), (13, True)]),\n",
    " 32: OrderedDict([(3, True), (5, True), (14, True), (18, False)]),\n",
    " 98: OrderedDict([(0, True), (4, False), (5, True), (8, True)]),\n",
    " 121: OrderedDict([#(0, True),вопрос проебан просто не вставлен в тест!!!!\n",
    "              (1, True),\n",
    "              (3, True),\n",
    "              (4, True),\n",
    "              (6, True),\n",
    "              (11, True)]),\n",
    " 130: OrderedDict([(0, True),\n",
    "              (1, True),\n",
    "             # (3, True),вопрос проебан просто не вставлен в тест!!!!\n",
    "              (4, True),\n",
    "              (5, False),\n",
    "              (8, True),\n",
    "              (9, True),\n",
    "              (16, True)]),\n",
    " 133: OrderedDict([(2, False),\n",
    "              (6, True),\n",
    "              (8, True),\n",
    "              (9, True),\n",
    "              (18, True),\n",
    "              (20, True),\n",
    "              (40, True),\n",
    "              (49, True)]),\n",
    " 200: OrderedDict([(0, True),(2, True), (4, True), (10, True), (12, True)]),\n",
    " 231: OrderedDict([(2, False), (4, False), (7, True), (12, True)]),\n",
    " 240: OrderedDict([(1, False), (11, True), (15, True)]),\n",
    " 316: OrderedDict([(0, True), (5, True), (7, True), (11, True)]),\n",
    " 331: OrderedDict([(2, False), (4, True), (10, True)]),\n",
    " 334: OrderedDict([(1, True), (2, True), (5, True), (8, True)]),\n",
    " 336: OrderedDict([(1, False), (7, True), (10, True), (13, True)]),\n",
    " 366: OrderedDict([(0, True),\n",
    "              (1, True),\n",
    "              (5, False),\n",
    "              (8, True),\n",
    "              (10, True),\n",
    "              (11, True),\n",
    "              (15, False)]),\n",
    " 371: OrderedDict([(0, True),\n",
    "              (2, True),\n",
    "              (3, True),\n",
    "              (6, True),\n",
    "              (12, True),\n",
    "              (21, True)])}#!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effected_collocations_start_indexes_list = [i for i in range(effected_collocations_start_indexes_list[0],effected_collocations_start_indexes_list[1]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_vector:\n",
    "    def __init__(self,debug = False):\n",
    "        self.debug = debug\n",
    "        self.vocab_features =[]\n",
    "        self.sentence_features =[]\n",
    "        #coreference_items, negation_items, sent_special_pos, dependencies_length, Y (answer)                   \n",
    "        self.text_fearues = [] #OrderedDict([(\"lix\",[]),(\"ttr\",[])])\n",
    "        self.answers_count = OrderedDict([(\"correct_answers\",0),(\"incorrect_answers\",0)])\n",
    "        self.collocations_list = []\n",
    "    def start_new_text(self):\n",
    "        self.answers_count['correct_answers'] = 0\n",
    "        self.answers_count['incorrect_answers'] = 0\n",
    "        if self.debug:print(\"answers count has been reset\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "    \n",
    "    def end_text(self, text_map):\n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"SUM UP TEXT VALUES\")\n",
    "            print(\"========\\n\")\n",
    "        correct_answers_rate = round(self.answers_count['correct_answers'] / (self.answers_count['correct_answers'] \n",
    "                                                                              + self.answers_count['incorrect_answers']),2)\n",
    "        current_text_features = []\n",
    "        if self.debug: \n",
    "            print(\"answers_count\", self.answers_count)\n",
    "        current_text_features.append(text_map['lix'])\n",
    "        current_text_features.append(text_map['ttr'])\n",
    "        current_text_features.append(text_map['sentences_count'])\n",
    "        current_text_features.append(text_map['average_sentence_length'])\n",
    "        #current_text_features.extend(text_map['vocab_properties'])\n",
    "        current_text_features.extend(text_map['sent_properties'])\n",
    "        current_text_features.append(correct_answers_rate)\n",
    "        self.text_fearues.append(current_text_features)\n",
    "        if self.debug: print(\"TEXT FEATURES\",self.text_fearues)\n",
    "    \n",
    "    def update_vector_with_answer_sentence(self, sentence_map, effected_collocations_start_indexes_list, correct_answer):\n",
    "        #update setnence and text features\n",
    "        if self.debug:\n",
    "            print(\"\\n===NEW REPLY CALCULATION====\")\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING SENTENCE RESULTS\")\n",
    "            print(\"========\\n\")\n",
    "            \n",
    "        #update setnence and text features\n",
    "        if correct_answer == True:\n",
    "            answer_value = 1\n",
    "            self.answers_count['correct_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is correct\")\n",
    "        else:\n",
    "            answer_value = 0\n",
    "            self.answers_count['incorrect_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is incorrect\")\n",
    "        if self.debug:print(\"check answers count\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "        current_sentence_features = []\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['negation'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['coreference'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['vozvr_verb'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['prich'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['deepr'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['case_complexity'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['mean_depend_length'])\n",
    "        \n",
    "        #current_sentence_features.extend(sentence_map['average_vocabulary'])\n",
    "        current_sentence_features.append(answer_value)#target variable\n",
    "        self.sentence_features.append(current_sentence_features)\n",
    "        \n",
    "        if self.debug: print(\"SENTENCE FEATURES\", current_sentence_features)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING VOCABULARY RESULTS\")\n",
    "            print(\"========\\n\") \n",
    "        effected_collocations_start_indexes_list_ranged = [i for i in range(effected_collocations_start_indexes_list[0],effected_collocations_start_indexes_list[1]+1)]    \n",
    "        for word_w in sentence_map['collocation_vectors_list']:\n",
    "            \n",
    "            if word_w[0] in effected_collocations_start_indexes_list_ranged:\n",
    "                if self.debug:print(word_w[1][0])\n",
    "                current_word_features = []\n",
    "                #current_word_features.append(str(\"local_freq_\") + str(word_w[1][1]))\n",
    "                current_word_features.append(word_w[1][1])\n",
    "                #print(\"11\", word_w[1][1])\n",
    "                #current_word_features.append(str(\"global_freq_mpi_ln_\") + glob_freq_log)\n",
    "                current_word_features.append(word_w[1][2])\n",
    "                #print(\"12\",word_w[1][2])\n",
    "                current_word_features.extend(word_w[2][0])\n",
    "                #print(\"20\",word_w[2][0])\n",
    "                current_word_features.append(answer_value)\n",
    "                \n",
    "                self.collocations_list.append(word_w[1][0])\n",
    "                #print(current_word_features)\n",
    "                self.vocab_features.append(current_word_features)\n",
    "    def export_user_db(self, learner_id):\n",
    "                \n",
    "        words_db = np.array([np.array(word) for word in self.vocab_features])\n",
    "        word_db_path = learner_id + '_word_db.csv'\n",
    "        np.savetxt(word_db_path, words_db, delimiter=',') #, fmt='%s'\n",
    "        \n",
    "        sentence_db = np.array([np.array(sent) for sent in self.sentence_features])\n",
    "        sentence_db_path = learner_id + '_sentence_db.csv'\n",
    "        np.savetxt(sentence_db_path, sentence_db, delimiter=',') \n",
    "        \n",
    "        text_db = np.array([np.array(text) for text in self.text_fearues])\n",
    "        text_db_path = learner_id + '_text_db.csv'\n",
    "        np.savetxt(text_db_path, text_db, delimiter=',') \n",
    "        \n",
    "        with open(learner_id + \"_colloc_lit.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "            for colloc in self.collocations_list:\n",
    "                f.write(colloc + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([0, 17, 26, 32, 98, 121, 130, 133, 200, 231, 240, 316, 331, 334, 336, 366, 371]),\n",
       " 17)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_maps_json_dict.keys(),len(list(text_maps_json_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text_sent_collocind_dict['0']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maps_json_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_index 0\n",
      "text_index 17\n",
      "text_index 26\n",
      "text_index 32\n",
      "text_index 98\n",
      "text_index 121\n",
      "text_index 130\n",
      "text_index 133\n",
      "text_index 200\n",
      "text_index 231\n",
      "text_index 240\n",
      "text_index 316\n",
      "text_index 331\n",
      "text_index 334\n",
      "text_index 336\n",
      "text_index 366\n",
      "text_index 371\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "user = user_vector(debug = DEBUG)\n",
    "for text in text_maps_json_dict.keys():\n",
    "    print(\"text_index\", text)\n",
    "    text_map = text_maps_json_dict[text]\n",
    "    user.start_new_text()\n",
    "    for question_sentence_index in question_text_sent_collocind_dict[str(text)].keys():\n",
    "\n",
    "        user.update_vector_with_answer_sentence(text_map['sentences_map'][int(question_sentence_index)], \n",
    "                                        effected_collocations_start_indexes_list = question_text_sent_collocind_dict[str(text)][str(question_sentence_index)],\n",
    "                                        correct_answer = ans_dict_enze[int(text)][int(question_sentence_index)])\n",
    "\n",
    "        user.end_text(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.export_user_db(\"enze_big_musician\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
