{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_and_recommendation import user_vector, recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing import get_text_map\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "import json\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРЕДОБРАБАТЫВАЕМ ТЕКСТЫ ПОДГОТОВЛЕННЫМ АЛГОРИТМОМ get_text_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts = []\n",
    "for file in listdir(\"./for_test\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        print(file)\n",
    "        line = ''\n",
    "        with open(join(\"./for_test\", file), 'r', encoding = \"utf-8\") as f:\n",
    "            for l in f.readlines():\n",
    "                line += l + ' '\n",
    "            line = line.strip()\n",
    "        #print(line)\n",
    "        text_map = get_text_map(join(\"./for_test\", file))\n",
    "        #text_map = get_text_map(line,raw_text_input = True)\n",
    "        with open(join(\"./for_test\", file) + \".json\", \"w\") as f:\n",
    "            json.dump(text_map,f, indent = 4, ensure_ascii = False) \n",
    "        texts.append(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_maps_json = []\n",
    "for file in listdir(\"./for_test\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open (join(\"./for_test\", file), \"r\", encoding='utf-8') as f:\n",
    "            text_map = json.load(f)\n",
    "            text_maps_json.append(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers count has been reset 0 0\n",
      "\n",
      "===NEW REPLY CALCULATION====\n",
      "\n",
      "========\n",
      "ADDING SENTENCE RESULTS\n",
      "========\n",
      "\n",
      "Answer for this question is correct\n",
      "check answers count 1 0\n",
      "SENTENCE FEATURES [0, 0, 0, 0, 0, 0, 0.25333333333333335, 1]\n",
      "\n",
      "========\n",
      "ADDING VOCABULARY RESULTS\n",
      "========\n",
      "\n",
      "0 в один из\n",
      "3 ночной клуб хмельницкий\n",
      "6 группа профессиональный боксер\n",
      "\n",
      "===NEW REPLY CALCULATION====\n",
      "\n",
      "========\n",
      "ADDING SENTENCE RESULTS\n",
      "========\n",
      "\n",
      "Answer for this question is incorrect\n",
      "check answers count 1 1\n",
      "SENTENCE FEATURES [0, 0, 0, 0, 0.04262100983035508, 0.09396631143511165, 0.29545454545454547, 0]\n",
      "\n",
      "========\n",
      "ADDING VOCABULARY RESULTS\n",
      "========\n",
      "\n",
      "0 как\n",
      "1 рассказывать в\n",
      "3 городской милиция повод\n",
      "6 для\n",
      "7 драка\n",
      "8 становиться\n"
     ]
    }
   ],
   "source": [
    "class user_vector:\n",
    "    def __init__(self,debug = False):\n",
    "        self.debug = debug\n",
    "        self.vocab_features =[]\n",
    "        self.sentence_features =[]\n",
    "        #coreference_items, negation_items, sent_special_pos, dependencies_length, Y (answer)                   \n",
    "        self.text_fearues = [] #OrderedDict([(\"lix\",[]),(\"ttr\",[])])\n",
    "        self.answers_count = OrderedDict([(\"correct_answers\",0),(\"incorrect_answers\",0)])\n",
    "        self.trigramms_list = []\n",
    "    def start_new_text(self):\n",
    "        self.answers_count['correct_answers'] = 0\n",
    "        self.answers_count['incorrect_answers'] = 0\n",
    "        if self.debug:print(\"answers count has been reset\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "    \n",
    "    def end_text(self, text_map):\n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"SUM UP TEXT VALUES\")\n",
    "            print(\"========\\n\")\n",
    "        correct_answers_rate = round(self.answers_count['correct_answers'] / (self.answers_count['correct_answers'] \n",
    "                                                                              + self.answers_count['incorrect_answers']),2)\n",
    "        current_text_features = []\n",
    "        if self.debug: \n",
    "            print(\"answers_count\", self.answers_count)\n",
    "        current_text_features.append(text_map['lix'])\n",
    "        current_text_features.append(text_map['ttr'])\n",
    "        #current_text_features.extend(text_map['vocab_properties'])\n",
    "        current_text_features.extend(text_map['sent_properties'])\n",
    "        current_text_features.append(correct_answers_rate)\n",
    "        self.text_fearues.append(current_text_features)\n",
    "        if self.debug: print(\"TEXT FEATURES\",self.text_fearues)\n",
    "    \n",
    "    def update_vector_with_answer_sentence(self, sentence_map, effected_collocations_start_indexes_list, correct_answer):\n",
    "        #update setnence and text features\n",
    "        if self.debug:\n",
    "            print(\"\\n===NEW REPLY CALCULATION====\")\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING SENTENCE RESULTS\")\n",
    "            print(\"========\\n\")\n",
    "            \n",
    "        #update setnence and text features\n",
    "        if correct_answer == True:\n",
    "            answer_value = 1\n",
    "            self.answers_count['correct_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is correct\")\n",
    "        else:\n",
    "            answer_value = 0\n",
    "            self.answers_count['incorrect_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is incorrect\")\n",
    "        if self.debug:print(\"check answers count\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "        current_sentence_features = []\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['negation'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['coreference'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['vozvr_verb'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['prich'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['deepr'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['case_complexity'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['mean_depend_length'])\n",
    "        #current_sentence_features.extend(sentence_map['average_vocabulary'])\n",
    "        current_sentence_features.append(answer_value)#target variable\n",
    "        self.sentence_features.append(current_sentence_features)\n",
    "        \n",
    "        if self.debug: print(\"SENTENCE FEATURES\", current_sentence_features)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING VOCABULARY RESULTS\")\n",
    "            print(\"========\\n\") \n",
    "            \n",
    "        for word_w in sentence_map['collocation_vectors_list']:\n",
    "            if word_w[0] in effected_collocations_start_indexes_list:\n",
    "                current_word_features = []\n",
    "                print(word_w[0],word_w[1][0])\n",
    "                current_word_features.append(str(\"local_freq_\") + str(word_w[1][1]))\n",
    "                current_word_features.append(str(\"global_freq_mpi_ln_\") + str(math.log(word_w[1][2])))\n",
    "                current_word_features.extend(word_w[2][0])\n",
    "                current_word_features.append(answer_value)\n",
    "                #print(current_word_features)\n",
    "                \n",
    "                \n",
    "DEBUG = True\n",
    "user = user_vector(debug = DEBUG)\n",
    "user.start_new_text()\n",
    "user.update_vector_with_answer_sentence(text_maps_json[0]['sentences_map'][0], \n",
    "                                        effected_collocations_start_indexes_list = [0,3,6],\n",
    "                                        correct_answer = True)\n",
    "user.update_vector_with_answer_sentence(text_maps_json[0]['sentences_map'][1], \n",
    "                                        effected_collocations_start_indexes_list = [0,1,3,6,7,8],\n",
    "                                        correct_answer = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "user = user_vector(debug = DEBUG)\n",
    "for text in texts:\n",
    "    #print(\"NEW TEXT\")\n",
    "    user.start_new_text()\n",
    "    answers = []  \n",
    "    \n",
    "    #print(len(text['sentences_map']))\n",
    "    mark_sentences_indexes = random.sample(range(len(text['sentences_map'])), min(len(text['sentences_map']),5))\n",
    "    #print(\"mark_sentences_indexes\", mark_sentences_indexes)\n",
    "    answers = [0,1,1,1,0]\n",
    "    \"\"\"\n",
    "    positive_count = random.randint(0, 4)\n",
    "    for i in range(positive_count):\n",
    "        answers[i] = 1\n",
    "    #for j in range(len(mark_sentences_indexes)): \n",
    "        #answers.append(random.randint(0, 1))\"\"\"\n",
    "    \n",
    "    for sentence_ind,answer in zip(mark_sentences_indexes,answers):\n",
    "        #print(sentence_ind)\n",
    "        user.update_vector_with_answer_sentence(text['sentences_map'][sentence_ind], correct_answer = answer)\n",
    "    user.end_text(text)\n",
    "vocab_model, sentence_model, text_model = user.export_user_vector()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_vector:\n",
    "    def __init__(self,debug = False):\n",
    "        self.debug = debug\n",
    "        self.vocab_features =[]\n",
    "        self.sentence_features =[]\n",
    "        #coreference_items, negation_items, sent_special_pos, dependencies_length, Y (answer)                   \n",
    "        self.text_fearues = [] #OrderedDict([(\"lix\",[]),(\"ttr\",[])])\n",
    "        self.answers_count = OrderedDict([(\"correct_answers\",0),(\"incorrect_answers\",0)])\n",
    "        self.trigramms_list = []\n",
    "        \n",
    "    def start_new_text(self):\n",
    "        self.answers_count['correct_answers'] = 0\n",
    "        self.answers_count['incorrect_answers'] = 0\n",
    "        if self.debug:print(\"answers count has been reset\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "        \n",
    "    def end_text(self, text_map):\n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"SUM UP TEXT VALUES\")\n",
    "            print(\"========\\n\")\n",
    "        correct_answers_rate = round(self.answers_count['correct_answers'] / (self.answers_count['correct_answers'] \n",
    "                                                                              + self.answers_count['incorrect_answers']),2)\n",
    "        current_text_features = []\n",
    "        if self.debug: \n",
    "            print(\"answers_count\", self.answers_count)\n",
    "        current_text_features.append(text_map['lix'])\n",
    "        current_text_features.append(text_map['ttr'])\n",
    "        #current_text_features.extend(text_map['vocab_properties'])\n",
    "        current_text_features.extend(text_map['sent_properties'])\n",
    "        current_text_features.append(correct_answers_rate)\n",
    "        self.text_fearues.append(current_text_features)\n",
    "        if self.debug: print(\"TEXT FEATURES\",self.text_fearues)\n",
    "        \n",
    "    def update_vector_with_answer_sentence(self, sentence_map, correct_answer):\n",
    "        #update setnence and text features\n",
    "        if self.debug:\n",
    "            print(\"\\n===NEW REPLY CALCULATION====\")\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING SENTENCE RESULTS\")\n",
    "            print(\"========\\n\")\n",
    "            \n",
    "        #update setnence and text features\n",
    "        if correct_answer == True:\n",
    "            answer_value = 1\n",
    "            self.answers_count['correct_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is correct\")\n",
    "        else:\n",
    "            answer_value = 0\n",
    "            self.answers_count['incorrect_answers'] += 1\n",
    "            if self.debug: print(\"Answer for this question is incorrect\")\n",
    "        if self.debug:print(\"check answers count\", self.answers_count['correct_answers'], self.answers_count['incorrect_answers'])\n",
    "        current_sentence_features = []\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['negation'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['coreference'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['vozvr_verb'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['prich'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['deepr'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['case_complexity'])\n",
    "        current_sentence_features.append(sentence_map['spec_sentence_features']['mean_depend_length'])\n",
    "        #current_sentence_features.extend(sentence_map['average_vocabulary'])\n",
    "        current_sentence_features.append(answer_value)#target variable\n",
    "        self.sentence_features.append(current_sentence_features)\n",
    "        \n",
    "        if self.debug: print(\"SENTENCE FEATURES\", current_sentence_features)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"\\n========\")\n",
    "            print(\"ADDING VOCABULARY RESULTS\")\n",
    "            print(\"========\\n\")\n",
    "            \n",
    "        understanding_importance_list = []\n",
    "        understanding_importance_sum = 0\n",
    "        for word_w in sentence_map['sentence_words']:\n",
    "            understanding_importance = word_w['vocabulary_prop']['tf_idf']\n",
    "            understanding_importance_sum += understanding_importance\n",
    "            understanding_importance_list.append([word_w['lemma'], understanding_importance,word_w['lex_vector'],word_w['lex_trigram']])\n",
    "            \n",
    "        for un_unit in understanding_importance_list:\n",
    "            if(understanding_importance_sum > 0):\n",
    "                un_unit[1] /= understanding_importance_sum\n",
    "        #if self.debug:print(\"understanding_importance_list\", understanding_importance_list)\n",
    "                       \n",
    "        \n",
    "        for unit_index in range(len(understanding_importance_list) ):\n",
    "            current_element = understanding_importance_list[unit_index][2]\n",
    "            \"\"\"\n",
    "            left_unit_index = unit_index - 1\n",
    "            if left_unit_index <0:\n",
    "                left_element = 300 * [0] \n",
    "            else:\n",
    "                left_element = understanding_importance_list[left_unit_index][2]\n",
    "            right_unit_index = unit_index + 1\n",
    "            if right_unit_index >=  len(understanding_importance_list):\n",
    "                right_element = 300 * [1]\n",
    "            else:\n",
    "                right_element = understanding_importance_list[right_unit_index][2]\n",
    "             \"\"\"   \n",
    "            current_lex_vector = []\n",
    "            #print(\"current_element\", current_element)\n",
    "            current_lex_vector.extend(current_element)\n",
    "            \n",
    "            if (correct_answer): \n",
    "                current_lex_vector.append(understanding_importance_list[unit_index][1])\n",
    "                self.trigramms_list.append(understanding_importance_list[unit_index][3])\n",
    "                #current_lex_vector.append(understanding_importance_list[unit_index][3])\n",
    "                self.vocab_features.append(current_lex_vector)\n",
    "                #print(\"current_lex_vector\", current_lex_vector)\n",
    "            else:\n",
    "                current_lex_vector.append(-1 * understanding_importance_list[unit_index][1])\n",
    "                self.trigramms_list.append(understanding_importance_list[unit_index][3])\n",
    "                #current_lex_vector.append(understanding_importance_list[unit_index][3])\n",
    "                self.vocab_features.append(current_lex_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СИМУЛИРУЕМ ПРОЦЕСС ПРОХОЖДЕНИЯ ТЕСТА НА ПРЕДОБРАБОТАННЫХ ТЕКСТАХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "user = user_vector(debug = DEBUG)\n",
    "for text in texts:\n",
    "    #print(\"NEW TEXT\")\n",
    "    user.start_new_text()\n",
    "    answers = []  \n",
    "    \n",
    "    #print(len(text['sentences_map']))\n",
    "    mark_sentences_indexes = random.sample(range(len(text['sentences_map'])), min(len(text['sentences_map']),5))\n",
    "    #print(\"mark_sentences_indexes\", mark_sentences_indexes)\n",
    "    answers = [0,1,1,1,0]\n",
    "    \"\"\"\n",
    "    positive_count = random.randint(0, 4)\n",
    "    for i in range(positive_count):\n",
    "        answers[i] = 1\n",
    "    #for j in range(len(mark_sentences_indexes)): \n",
    "        #answers.append(random.randint(0, 1))\"\"\"\n",
    "    \n",
    "    for sentence_ind,answer in zip(mark_sentences_indexes,answers):\n",
    "        #print(sentence_ind)\n",
    "        user.update_vector_with_answer_sentence(text['sentences_map'][sentence_ind], correct_answer = answer)\n",
    "    user.end_text(text)\n",
    "vocab_model, sentence_model, text_model = user.export_user_vector()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "СТРОИМ МОДЕЛЬ НОВОГО ТЕКСТА ПОД РЕКОМЕНДАЦИЮ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_8.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "    for l in f.readlines():\n",
    "        line += l + ' '\n",
    "    line = line.strip()\n",
    "    text_map = get_text_map(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommendation(texts[8],vocab_model,sentence_model,text_model, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(%понимаемых слов - 0.85, показатель уверенности в предсказании)\n",
    "средняя вероятность понимания предложения - 0.85\n",
    "%вопросом на которые человек сможет ответить - 0.85"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
