{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import FastTextKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastTextKeyedVectors.load(\"D:/fasttext_word2vec/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fasttext.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext['лепин']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectors.txt\", \"w+\", encoding = \"utf-8\") as f:\n",
    "    for word in fasttext.wv.vocab.keys():\n",
    "        f.write(str(word) + ' ' + str(fasttext[word]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>texts_3000</th>\n",
       "      <th>same_word_list_3000</th>\n",
       "      <th>percent_3000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Указом президента России Бориса Ельцина внесен...</td>\n",
       "      <td>{'отмечать', 'изменение', 'иностранный', 'роль...</td>\n",
       "      <td>71,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>В Харькове неизвестные изуродовали лицо депута...</td>\n",
       "      <td>{'регион', 'серьезный', 'агентство', 'являться...</td>\n",
       "      <td>72,22222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Руководитель  следствия по делу  \"Аэрофлота\"  ...</td>\n",
       "      <td>{'ставить', 'ход', 'фирма', 'отказываться', 'с...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Постоянный экономический рост позволит Китаю к...</td>\n",
       "      <td>{'внутренний', 'момент', 'сфера', 'слово', 'мн...</td>\n",
       "      <td>72,60273973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>В одном из ночных клубов Хмельницкого группа п...</td>\n",
       "      <td>{'отмечать', 'повод', 'смерть', 'свидетель', '...</td>\n",
       "      <td>70,68965517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         texts_3000  \\\n",
       "0           0  Указом президента России Бориса Ельцина внесен...   \n",
       "1           1  В Харькове неизвестные изуродовали лицо депута...   \n",
       "2           2  Руководитель  следствия по делу  \"Аэрофлота\"  ...   \n",
       "3           3  Постоянный экономический рост позволит Китаю к...   \n",
       "4           4  В одном из ночных клубов Хмельницкого группа п...   \n",
       "\n",
       "                                 same_word_list_3000 percent_3000  \n",
       "0  {'отмечать', 'изменение', 'иностранный', 'роль...        71,25  \n",
       "1  {'регион', 'серьезный', 'агентство', 'являться...  72,22222222  \n",
       "2  {'ставить', 'ход', 'фирма', 'отказываться', 'с...           74  \n",
       "3  {'внутренний', 'момент', 'сфера', 'слово', 'мн...  72,60273973  \n",
       "4  {'отмечать', 'повод', 'смерть', 'свидетель', '...  70,68965517  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = pd.read_csv(\"3000.csv\")\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from text_processing_udpipe_w2v import get_text_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 898 \n",
      "\n",
      "property found for ind =  5\n",
      "[-1, -1, -1, -1, -1, 0, -1]\n",
      "property found for ind =  0\n",
      "[1, -1, -1, -1, -1, 0, -1]\n",
      "property found for ind =  1\n",
      "[1, 2, -1, -1, -1, 0, -1]\n",
      "property found for ind =  6\n",
      "[1, 2, -1, -1, -1, 0, 3]\n",
      "10 / 898 \n",
      "\n",
      "20 / 898 \n",
      "\n",
      "30 / 898 \n",
      "\n",
      "40 / 898 \n",
      "\n",
      "50 / 898 \n",
      "\n",
      "60 / 898 \n",
      "\n",
      "70 / 898 \n",
      "\n",
      "80 / 898 \n",
      "\n",
      "90 / 898 \n",
      "\n",
      "100 / 898 \n",
      "\n",
      "110 / 898 \n",
      "\n",
      "120 / 898 \n",
      "\n",
      "130 / 898 \n",
      "\n",
      "140 / 898 \n",
      "\n",
      "150 / 898 \n",
      "\n",
      "160 / 898 \n",
      "\n",
      "170 / 898 \n",
      "\n",
      "180 / 898 \n",
      "\n",
      "190 / 898 \n",
      "\n",
      "200 / 898 \n",
      "\n",
      "210 / 898 \n",
      "\n",
      "220 / 898 \n",
      "\n",
      "230 / 898 \n",
      "\n",
      "240 / 898 \n",
      "\n",
      "250 / 898 \n",
      "\n",
      "260 / 898 \n",
      "\n",
      "270 / 898 \n",
      "\n",
      "280 / 898 \n",
      "\n",
      "290 / 898 \n",
      "\n",
      "300 / 898 \n",
      "\n",
      "310 / 898 \n",
      "\n",
      "320 / 898 \n",
      "\n",
      "330 / 898 \n",
      "\n",
      "340 / 898 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'27'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e96df4c04b2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext_ind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtext_ind\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtext_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"texts_3000\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_text_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msentence_prop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent_properties'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent_ind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Autotutor\\processing_test_recommend\\text_processing_udpipe_w2v.py\u001b[0m in \u001b[0;36mget_text_map\u001b[1;34m(text, raw_text_input)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[0mtf_idf_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tf_idf_dict\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlemm_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[0mtext_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconllu_text_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_idf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m     \u001b[0msentence_map_dep\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconllu_text_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m     \u001b[0msentence_map_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_with_lex_vector\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence_map_dep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[0msentence_map_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_map_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Autotutor\\processing_test_recommend\\text_processing_udpipe_w2v.py\u001b[0m in \u001b[0;36mget_dependencies\u001b[1;34m(conllu_map, text_map_input)\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_nominal_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                     \u001b[0mcurrent_element_real_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnominal2real_index_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m                     \u001b[0mhead_element_real_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnominal2real_index_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhead_nominal_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m                     \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_element_real_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhead_element_real_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[0mdistances_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '27'"
     ]
    }
   ],
   "source": [
    "texts_indexes = [-1] * 7\n",
    "for text_ind in range(len(texts)):\n",
    "    if text_ind%10 == 0: print(text_ind,\"/\", len(texts),\"\\n\")\n",
    "    text_map = get_text_map(texts.iloc[text_ind][\"texts_3000\"], raw_text_input = True)\n",
    "    sentence_prop = text_map['sent_properties']\n",
    "    for sent_ind in range(len(sentence_prop)):\n",
    "        if texts_indexes[sent_ind] == -1 and sentence_prop[sent_ind] > 0.1:\n",
    "            print(\"property found for ind = \", sent_ind)\n",
    "            texts_indexes[sent_ind] = text_ind\n",
    "            print(texts_indexes)\n",
    "            break\n",
    "    go_on_searching = False\n",
    "    for el in texts_indexes:\n",
    "        if(el == -1):\n",
    "            go_on_searching = True\n",
    "            break\n",
    "    if not go_on_searching:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-1 - ok, -1 - ok, -1 - bad, -1 - bad, -1 - bad, -1 -soso, 0 -soso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_map(texts.iloc[1][\"texts_3000\"], raw_text_input = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
