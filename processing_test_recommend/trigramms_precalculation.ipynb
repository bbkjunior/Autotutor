{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import operator\n",
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "\n",
    "from text_processing_udpipe_w2v import get_text_map\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statistics \n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>texts_3000</th>\n",
       "      <th>same_word_list_3000</th>\n",
       "      <th>percent_3000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Указом президента России Бориса Ельцина внесен...</td>\n",
       "      <td>{'отмечать', 'изменение', 'иностранный', 'роль...</td>\n",
       "      <td>71,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>В Харькове неизвестные изуродовали лицо депута...</td>\n",
       "      <td>{'регион', 'серьезный', 'агентство', 'являться...</td>\n",
       "      <td>72,22222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Руководитель  следствия по делу  \"Аэрофлота\"  ...</td>\n",
       "      <td>{'ставить', 'ход', 'фирма', 'отказываться', 'с...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Постоянный экономический рост позволит Китаю к...</td>\n",
       "      <td>{'внутренний', 'момент', 'сфера', 'слово', 'мн...</td>\n",
       "      <td>72,60273973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>В одном из ночных клубов Хмельницкого группа п...</td>\n",
       "      <td>{'отмечать', 'повод', 'смерть', 'свидетель', '...</td>\n",
       "      <td>70,68965517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         texts_3000  \\\n",
       "0           0  Указом президента России Бориса Ельцина внесен...   \n",
       "1           1  В Харькове неизвестные изуродовали лицо депута...   \n",
       "2           2  Руководитель  следствия по делу  \"Аэрофлота\"  ...   \n",
       "3           3  Постоянный экономический рост позволит Китаю к...   \n",
       "4           4  В одном из ночных клубов Хмельницкого группа п...   \n",
       "\n",
       "                                 same_word_list_3000 percent_3000  \n",
       "0  {'отмечать', 'изменение', 'иностранный', 'роль...        71,25  \n",
       "1  {'регион', 'серьезный', 'агентство', 'являться...  72,22222222  \n",
       "2  {'ставить', 'ход', 'фирма', 'отказываться', 'с...           74  \n",
       "3  {'внутренний', 'момент', 'сфера', 'слово', 'мн...  72,60273973  \n",
       "4  {'отмечать', 'повод', 'смерть', 'свидетель', '...  70,68965517  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_300 = pd.read_csv(\"3000.csv\", sep = \",\")\n",
    "dp_300.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025718</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>-0.015405</td>\n",
       "      <td>-0.078441</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017842</td>\n",
       "      <td>-0.041569</td>\n",
       "      <td>0.077341</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>-0.037332</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>-0.036526</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>-0.030144</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>-0.038640</td>\n",
       "      <td>-0.008778</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>-0.069830</td>\n",
       "      <td>0.107995</td>\n",
       "      <td>0.041295</td>\n",
       "      <td>0.074002</td>\n",
       "      <td>-0.027547</td>\n",
       "      <td>-0.023377</td>\n",
       "      <td>-0.026141</td>\n",
       "      <td>-0.028869</td>\n",
       "      <td>-0.062423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030954</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>-0.013010</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>-0.015895</td>\n",
       "      <td>-0.004156</td>\n",
       "      <td>0.043479</td>\n",
       "      <td>-0.097470</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012979</td>\n",
       "      <td>-0.054481</td>\n",
       "      <td>0.129445</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.064443</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>-0.068905</td>\n",
       "      <td>-0.063734</td>\n",
       "      <td>-0.017192</td>\n",
       "      <td>-0.050362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029982</td>\n",
       "      <td>0.046834</td>\n",
       "      <td>-0.016939</td>\n",
       "      <td>0.034865</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>-0.025411</td>\n",
       "      <td>0.057243</td>\n",
       "      <td>-0.126531</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>-0.012146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063004</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.101894</td>\n",
       "      <td>0.074595</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>-0.022268</td>\n",
       "      <td>-0.048629</td>\n",
       "      <td>-0.074751</td>\n",
       "      <td>0.018179</td>\n",
       "      <td>-0.035168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023059</td>\n",
       "      <td>0.052791</td>\n",
       "      <td>-0.031716</td>\n",
       "      <td>0.051026</td>\n",
       "      <td>-0.040494</td>\n",
       "      <td>-0.007667</td>\n",
       "      <td>0.052596</td>\n",
       "      <td>-0.087467</td>\n",
       "      <td>-0.004326</td>\n",
       "      <td>0.018472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052820</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.062752</td>\n",
       "      <td>0.100702</td>\n",
       "      <td>0.030436</td>\n",
       "      <td>0.018350</td>\n",
       "      <td>-0.031551</td>\n",
       "      <td>-0.050780</td>\n",
       "      <td>0.061213</td>\n",
       "      <td>-0.059490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.025718 -0.004776  0.031789  0.018760 -0.015405 -0.078441 -0.002449   \n",
       "1  0.018746  0.003426  0.020213  0.056091 -0.006876 -0.030144  0.021508   \n",
       "2  0.030954  0.012660 -0.013010  0.058922 -0.015895 -0.004156  0.043479   \n",
       "3  0.029982  0.046834 -0.016939  0.034865 -0.004499 -0.025411  0.057243   \n",
       "4  0.023059  0.052791 -0.031716  0.051026 -0.040494 -0.007667  0.052596   \n",
       "\n",
       "        7         8         9      ...          291       292       293  \\\n",
       "0 -0.008062 -0.001146  0.010007    ...     0.017842 -0.041569  0.077341   \n",
       "1 -0.038640 -0.008778  0.014135    ...     0.000866 -0.069830  0.107995   \n",
       "2 -0.097470  0.013334 -0.000223    ...    -0.012979 -0.054481  0.129445   \n",
       "3 -0.126531  0.015994 -0.012146    ...    -0.063004  0.012827  0.101894   \n",
       "4 -0.087467 -0.004326  0.018472    ...    -0.052820  0.033877  0.062752   \n",
       "\n",
       "        294       295       296       297       298       299       300  \n",
       "0  0.012496  0.049400 -0.037332  0.001658  0.010653 -0.036526 -0.000000  \n",
       "1  0.041295  0.074002 -0.027547 -0.023377 -0.026141 -0.028869 -0.062423  \n",
       "2  0.047809  0.064443 -0.009311 -0.068905 -0.063734 -0.017192 -0.050362  \n",
       "3  0.074595  0.044913 -0.022268 -0.048629 -0.074751  0.018179 -0.035168  \n",
       "4  0.100702  0.030436  0.018350 -0.031551 -0.050780  0.061213 -0.059490  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dp = pd.read_csv(\"word_db.csv\", sep = \",\", header=None)\n",
    "word_dp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramms =[]\n",
    "with open (\"trigramm_db.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for trig in f.readlines():\n",
    "        trigramms.append(trig[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastTextKeyedVectors.load(\"D:/fasttext_word2vec/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_recommended_text_map = get_text_map(\"text_8.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramms_recommendation_store (rec_text_map, marked_trigramms_db, trigrmms_list):\n",
    "    sentences_map = rec_text_map['sentences_map']\n",
    "    rec_text_dict = {}#key = trigr from recommended text value = list of similar trigramms from marked db\n",
    "    for sentence_ind in range(len(sentences_map)):\n",
    "        for word in sentences_map[sentence_ind]['sentence_words']:\n",
    "            text_word_vec = word['lex_vector'].reshape(1, -1)\n",
    "            text_word_dist_dict = {}\n",
    "            for word_db_ind in range(len(marked_trigramms_db)):\n",
    "                marked_trigramms_db_word_vec = marked_trigramms_db.iloc[word_db_ind][:-1].values.reshape(1, -1)\n",
    "                similarity = cosine_similarity(text_word_vec,marked_trigramms_db_word_vec)\n",
    "                text_word_dist_dict[word_db_ind] = similarity[0][0]\n",
    "           \n",
    "            sorted_x = sorted(text_word_dist_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "            filtered_sorted_list = filter_similar_trigramms(sorted_x)\n",
    "            if not filtered_sorted_list:\n",
    "                rec_text_dict[word['lex_trigram']] = None\n",
    "            else:\n",
    "                similar_trigrams= []\n",
    "                for similar_el in filtered_sorted_list:\n",
    "                    similar_trigrams.append([similar_el, trigrmms_list[similar_el[0]],abs(list(marked_trigramms_db.iloc[similar_el[0]])[-1])])#None - в зависимости от \n",
    "                rec_text_dict[word['lex_trigram']] = similar_trigrams\n",
    "    return rec_text_dict\n",
    "#ПОЛСЕ ТОГО КАК МЫ ПОЛУЧИЛИ ОТВЕТЫ НА ВОПРОСЫ ФОРМИРУЕТСЯ ДАТАСЕТ С ЦЕЛЕВОЙ ПЕРЕМЕННОЙ\n",
    "#СООТВЕТТСВЕННО МОЖНО ПРОСТО ПРОЙТИСЬ ПО ВСЕМУ НАКОПЛЕННОМУ СЛОВАРЮ И ПРОСТО ПРОСТАВИТЬ МИНУСЫ ТАМ ГДЕ НАДО (ПО ИНДЕКСАМ ) АДАЛЕЕ ПРОДОЛЖИТЬ ПОДСЧЕТ\n",
    "#STORE_JSON  = trigramms_recommendation_store(potential_recommended_text_map,word_dp,  trigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['222',\n",
       " '862',\n",
       " '321',\n",
       " '364',\n",
       " '502',\n",
       " '878',\n",
       " '666',\n",
       " '92',\n",
       " '615',\n",
       " '450',\n",
       " '722',\n",
       " '732',\n",
       " '611',\n",
       " '251']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict = {\"222\" :['0-' ,'1-' ,'2+' ,'4+'],\n",
    "\"862\":['1+','2-', '4+', '6-', '12-'],\n",
    "\"321\":['5+' ,'0-' ,'4-' ,'1+'],\n",
    "\"364\":['2+', '6-', '3-', '7-' ],\n",
    "\"502\":['13-', '3+', '0-', '4+'],\n",
    "\"878\":['3-', '0+', '4-', '1-'],\n",
    "\"666\":['3-', '1-', '2-' ],\n",
    "\"92\":['1+', '2-'],\n",
    "\"615\":['1+', '2-'],\n",
    "\"450\":['6-', '7-', '11+', '9+' ,'10+' ],\n",
    "\"722\":['1+', '6-', '2+', '3+', '6-', '8-'],\n",
    "\"732\":['8-', '0-', '3-', '4-', '2+', '1-'],\n",
    "\"611\":['1+'],\n",
    "\"251\":['5-', '4+', '2+', '1+', '3+']}\n",
    "list(answer_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./texts_json/data.json',  'w', encoding = \"utf-8\") as outfile:  \n",
    "    json.dump(STORE_JSON, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "СОСТАВИТЬ СООТВЕТСТВИЕ (ПРЕДЛОЖЕНИЕ - ИНДЕКС ТРИГРАММЫ В БАЗЕ ДАННЫХ) И РАЗМЕЧАТЬ СООТВЕТСТВУЮЩИМ ОБРАОМ ПОСЛЕ ПОЛУЧЕНИЯ ОТВЕТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_recommended_text_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 599/599 [22:33:06<00:00, 135.54s/it]\n"
     ]
    }
   ],
   "source": [
    "for text_ind in tqdm(range(299,len(dp_300))):\n",
    "    if text_ind not in list(answer_dict.keys()):\n",
    "        potential_recommended_text_map = get_text_map(dp_300.iloc[text_ind]['texts_3000'], raw_text_input= True)\n",
    "        STORE_JSON  = trigramms_recommendation_store(potential_recommended_text_map,word_dp,trigramms)\n",
    "        save_loc = './texts_json/text_' + str(text_ind) + \".json\"\n",
    "        with open(save_loc,  'w', encoding = \"utf-8\") as outfile:  \n",
    "            json.dump(STORE_JSON, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigramms_recommendation (rec_text_map, marked_trigramms_db, trigrmms_list):\n",
    "    sentences_map = rec_text_map['sentences_map']\n",
    "    calc_text_dict = {}\n",
    "    for sentence_ind in tqdm(range(len(sentences_map))):\n",
    "        for word in sentences_map[sentence_ind]['sentence_words']:\n",
    "            text_word_vec = word['lex_vector'].reshape(1, -1)\n",
    "            text_word_dist_dict = {}\n",
    "            for word_db_ind in range(len(marked_trigramms_db)):\n",
    "                marked_trigramms_db_word_vec = marked_trigramms_db.iloc[word_db_ind][:-1].values.reshape(1, -1)\n",
    "                similarity = cosine_similarity(text_word_vec,marked_trigramms_db_word_vec)\n",
    "                text_word_dist_dict[word_db_ind] = similarity\n",
    "           \n",
    "            sorted_x = sorted(text_word_dist_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "            filtered_sorted_list = filter_similar_trigramms(sorted_x)\n",
    "            if not filtered_sorted_list:\n",
    "                calc_text_dict[word['lex_trigram']] = None\n",
    "            else:\n",
    "                similar_trigrams= []\n",
    "                for similar_el in filtered_sorted_list:\n",
    "                    similar_trigrams.append((similar_el, trigrmms_list[similar_el[0]],list(marked_trigramms_db.iloc[similar_el[0]])[-1] ))\n",
    "                calc_text_dict[word['lex_trigram']] = similar_trigrams\n",
    "                #=====ЗДЕСЬ ПР ПОДГРУЗКЕ НАДО БУДЕТ РАЗМЕТИТЬ ЦЕЛЕВУЮ ПЕРЕМЕННУЮВ СООТВТЕТСВИИ С КОРРЕКТНОСТЬЮ ОТВЕТА И ДАЛЕЕ ПЛЯСАТЬ\n",
    "                \n",
    "    vocab_recommendation_dict = {}\n",
    "    for key in calc_text_dict.keys():\n",
    "        print(key , \"===>\", calc_text_dict[key],'\\n')\n",
    "        understdanding_prob_list = []\n",
    "        if (calc_text_dict[key]):\n",
    "            for trgrm in calc_text_dict[key]:\n",
    "                understdanding_prob = trgrm[2]\n",
    "                understdanding_prob_list.append(understdanding_prob)\n",
    "            understdanding_prob_mean = statistics.mean(understdanding_prob_list) \n",
    "            vocab_recommendation_dict[key] = understdanding_prob\n",
    "    positive_understanding_prob = 0\n",
    "    whole_understading_prob = 0 \n",
    "    for key in vocab_recommendation_dict.keys():\n",
    "        print(key , \"===>\", vocab_recommendation_dict[key],'\\n')\n",
    "        if (vocab_recommendation_dict[key] > 0):\n",
    "            positive_understanding_prob += vocab_recommendation_dict[key]\n",
    "            whole_understading_prob += abs(vocab_recommendation_dict[key])\n",
    "        elif(vocab_recommendation_dict[key] < 0):\n",
    "            whole_understading_prob += abs(vocab_recommendation_dict[key])\n",
    "    understanding_prob = positive_understanding_prob/whole_understading_prob\n",
    "    print(\"understanding_prob\",understanding_prob)\n",
    "        \n",
    "trigramms_recommendation(potential_recommended_text_map,word_dp,  trigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dp = pd.read_csv(\"sentence_db.csv\", sep = \",\", header=None)\n",
    "sent_dp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db = pd.read_csv(\"text_db.csv\", sep = \",\", header=None)\n",
    "text_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_text_features_recommendation (rec_text_map, marked_sentences_db, marked_text_db):\n",
    "    rec_text_features = []\n",
    "    rec_text_features.append(rec_text_map['lix'])\n",
    "    rec_text_features.append(rec_text_map['ttr'])\n",
    "    rec_text_features.extend(rec_text_map['sent_properties'])\n",
    "    rec_text_vec = np.array(rec_text_features).reshape(1, -1)\n",
    "    #print(rec_text_vec)\n",
    "    text_similarity_dict = {}\n",
    "    for text_similarity_ind in range(len(marked_text_db)):\n",
    "        text_vec = marked_text_db.iloc[text_similarity_ind][:-1].values.reshape(1, -1)\n",
    "        #print(text_vec)\n",
    "        similarity = cosine_similarity(rec_text_vec,text_vec)\n",
    "        #print(similarity)\n",
    "        text_similarity_dict[text_similarity_ind] = similarity\n",
    "    sorted_text_feat_dict = sorted(text_similarity_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    text_feeat_mean_dict = []\n",
    "    for text_el in sorted_text_feat_dict[:min(3, len(sorted_text_feat_dict))]:\n",
    "        understandng = list(marked_text_db.iloc[text_el[0]])[-1]\n",
    "        text_feeat_mean_dict.append(understandng)\n",
    "    text_feeat_mean = statistics.mean(text_feeat_mean_dict)\n",
    "    \n",
    "    #print(sorted_text_feat_dict)\n",
    "    print(\"text_understanding_prob\" , text_feeat_mean)\n",
    "    \n",
    "    sentences_understanding = []\n",
    "    for sentence in rec_text_map['sentences_map']:\n",
    "        rec_sent_feat = []\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['negation'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['coreference'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['vozvr_verb'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['prich'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['deepr'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['case_complexity'])\n",
    "        rec_sent_feat.append(sentence['spec_sentence_features']['mean_depend_length'])\n",
    "        rec_sent_vec = np.array(rec_sent_feat).reshape(1, -1)\n",
    "        sent_similarity_dict = {}\n",
    "        for sent_ind in range(len(marked_sentences_db)):\n",
    "            sent_vec = marked_sentences_db.iloc[sent_ind][:-1].values.reshape(1, -1)\n",
    "            similarity = cosine_similarity(rec_sent_vec,sent_vec)\n",
    "            sent_similarity_dict[sent_ind] = similarity\n",
    "        sorted_sent_similarity_dict = sorted(sent_similarity_dict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "        sent_similarity_list = []\n",
    "        for sim_sent in sorted_sent_similarity_dict[:3]:\n",
    "            #print(sim_sent)\n",
    "            sent_undestanding_mark = list(marked_sentences_db.iloc[sim_sent[0]])[-1]\n",
    "            sent_similarity_list.append(sent_undestanding_mark)\n",
    "        #print(sent_similarity_list)\n",
    "        sent_similarity_mean = statistics.mean(sent_similarity_list)\n",
    "        #print(sent_similarity_mean)\n",
    "        sentences_understanding.append(sent_similarity_mean)\n",
    "    sentences_understanding_mean = statistics.mean(sentences_understanding)\n",
    "    print(\"sentences_understanding_mean\", sentences_understanding_mean)\n",
    "             \n",
    "sent_text_features_recommendation(potential_recommended_text_map, marked_text_db = text_db,marked_sentences_db =  sent_dp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_recommended_text_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
