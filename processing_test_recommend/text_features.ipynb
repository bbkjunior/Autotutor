{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–,»«'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "full_punctuation = punctuation + \"–\" + \",\" + \"»\" + \"«\"\n",
    "full_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('russian.pickle')\n",
    "text = \"Ай да А.С. Пушкин! Ай да сукин сын!\"\n",
    "print(\"Before:\", nltk.sent_tokenize(text))\n",
    "print(\"After:\", tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    clean_sentence = []\n",
    "    #print(sentences)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        clean_text= ''\n",
    "        for word in words:\n",
    "            clean_word = ''\n",
    "            for char in word:\n",
    "                if char != \" \" and char not in full_punctuation:\n",
    "                    clean_word += char.lower()\n",
    "            clean_text += clean_word + ' '\n",
    "        clean_text = re.sub(' +', ' ', clean_text)\n",
    "        clean_text = clean_text.rstrip()\n",
    "        clean_text = clean_text.lstrip()\n",
    "        clean_sentence.append(clean_text)\n",
    "    return clean_sentence\n",
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens #if token not in russian_stopwords\\\n",
    "              if token != \" \" and token.strip() not in full_punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\"\"\"\n",
    "\n",
    "def process_and_write_file(file, destination):\n",
    "    preprocessed_text = []\n",
    "    lines = 0\n",
    "    with open (file, \"r\", encoding = \"utf-8\") as file:\n",
    "        all_lines = file.readlines()\n",
    "        bar = progressbar.ProgressBar(maxval=len(all_lines),\n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        bar.start()\n",
    "        for line in all_lines:\n",
    "            t = preprocess_text(line)\n",
    "            #print(t)\n",
    "            preprocessed_text.extend(t)\n",
    "            lines += 1\n",
    "            bar.update(lines)\n",
    "            sleep(0.1)               \n",
    "            \n",
    "    with open (destination, \"w+\", encoding = \"utf-8\") as write_file:\n",
    "        for sentence in preprocessed_text:\n",
    "            #print(\"write\", sentence)\n",
    "            write_file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 93,
=======
   "execution_count": null,
>>>>>>> 98002302fa47064680e33245161e9197d6e9f8a9
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_write_file(\"text_8.txt\", \"text_8_processed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIX"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": null,
>>>>>>> 98002302fa47064680e33245161e9197d6e9f8a9
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lix(processed_text):\n",
    "    with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "        sentences_count = len(sentences)\n",
    "        words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "        long_words_count = 0 #more than 6\n",
    "        for line in sentences:\n",
    "            for word in line.split():\n",
    "                if len(word) > 6:\n",
    "                    long_words_count += 1\n",
    "\n",
    "        lix = words_count/ sentences_count + (long_words_count * 100) / words_count\n",
    "        #print(long_words_count)\n",
    "        #print(words_count)\n",
    "        #print(len(sentences))\n",
    "        print(lix)\n",
    "calculate_lix(\"text_8_processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Максимальная длина слова в морфемах morph/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"text_8_processed.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    sentences = file.readlines()\n",
    "    words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "    for sentence in sentences:\n",
    "        mystemmed_line = mystem.analyze(sentence)\n",
    "        for mystemmed_word in mystemmed_line:\n",
    "            keys = list(mystemmed_word.keys())\n",
    "            values = list(mystemmed_word.values())\n",
    "            word  = mystemmed_word['text']\n",
    "            if ('analysis' not in keys):\n",
    "                pass\n",
    "            elif(mystemmed_word['analysis'] ==[]):\n",
    "                pass\n",
    "            else:\n",
    "                grammar = mystemmed_word['analysis'][0]['gr']\n",
    "                grammar_sep_by_comma = grammar.split(',')\n",
    "                print(grammar_sep_by_comma[0])\n",
    "                if (len(grammar_sep_by_comma) == 1):\n",
    "                    pass\n",
    "                else:\n",
    "                    pos = re.match('[A-Z\\s]+', grammar_sep_by_comma[0])[0]\n",
    "                    print(pos)\n",
    "                    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-Token Ratio"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-b72a90702feb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[A-Z\\s]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrammar_sep_by_comma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcalculate_lix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text_8_processed.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-b72a90702feb>\u001b[0m in \u001b[0;36mcalculate_lix\u001b[1;34m(processed_text)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mwords_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mmystemmed_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmystemmed_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmystemmed_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystemmed_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_mystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
>>>>>>> 98002302fa47064680e33245161e9197d6e9f8a9
   "source": [
    "def tokenize_text(processed_text, destination):\n",
    "    mystem = Mystem()\n",
    "    with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "        clean_sentences = []\n",
    "        for sentence in sentences:\n",
    "            clean_sentence = ''\n",
    "            mystemmed_line = mystem.analyze(sentence)\n",
    "            for mystemmed_word in mystemmed_line:\n",
    "                keys = list(mystemmed_word.keys())\n",
    "                values = list(mystemmed_word.values())\n",
    "                word  = mystemmed_word['text']\n",
    "                if ('analysis' not in keys):\n",
    "                    pass\n",
    "                elif(mystemmed_word['analysis'] ==[]):\n",
    "                    pass\n",
    "                else:\n",
<<<<<<< HEAD
    "                    grammar = mystemmed_word['analysis'][0]['gr']\n",
    "                    grammar_sep_by_comma = grammar.split(',')\n",
    "                    rint(grammar_sep_by_comma[0])\n",
    "                    if (len(grammar_sep_by_comma) == 1):\n",
    "                        pass\n",
    "                    else:\n",
    "                        pos = re.match('[A-Z\\s]+', grammar_sep_by_comma[0])[0]\n",
    "                        print(pos)\n",
    "calculate_lix(\"text_8_processed.txt\")"
=======
    "                    lemma = mystemmed_word['analysis'][0]['lex']\n",
    "                    clean_sentence += lemma +' '\n",
    "            clean_sentence = clean_sentence.rstrip()\n",
    "            clean_sentences.append(clean_sentence)\n",
    "            \n",
    "            \n",
    "    with open (destination, \"w+\", encoding = \"utf-8\") as write_file:\n",
    "        for sentence in clean_sentences:\n",
    "            #print(\"write\", sentence)\n",
    "            write_file.write(sentence + '\\n')\n",
    "\n"
>>>>>>> 98002302fa47064680e33245161e9197d6e9f8a9
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(\"text_8_processed.txt\", \"text_8_tokenized.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'gr': 'V,несов,пе=непрош,ед,изъяв,1-л',\n",
       "    'lex': 'делать',\n",
       "    'wt': 1}],\n",
       "  'text': 'делаю'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'gr': 'V,несов,пе=(непрош,мн,изъяв,1-л|непрош,ед,прич,кр,муж,страд)',\n",
       "    'lex': 'делать',\n",
       "    'wt': 1}],\n",
       "  'text': 'делаем'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'gr': 'V=(непрош,ед,изъяв,2-л,сов|ед,пов,2-л,сов)',\n",
       "    'lex': 'деаедать',\n",
       "    'qual': 'bastard',\n",
       "    'wt': 1}],\n",
       "  'text': 'деаешь'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'gr': 'V,несов,пе=непрош,мн,изъяв,2-л',\n",
       "    'lex': 'делать',\n",
       "    'wt': 1}],\n",
       "  'text': 'делаете'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'gr': 'V,несов,пе=непрош,мн,изъяв,3-л',\n",
       "    'lex': 'делать',\n",
       "    'wt': 1}],\n",
       "  'text': 'делают'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'gr': 'S,муж,од=дат,мн',\n",
       "    'lex': 'спорстмен',\n",
       "    'qual': 'bastard',\n",
       "    'wt': 0.6399231727}],\n",
       "  'text': 'спорстменам'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Mystem()\n",
    "m.analyze(\"делаю делаем деаешь делаете делают спорстменам\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_type_token_ratio(tokenized_text):\n",
    "    with open (tokenized_text, \"r\", encoding = \"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "        pos_list = []\n",
    "        words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "        all_words = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                all_words.append(word)\n",
    "                \n",
    "        unqie_words = set(all_words)\n",
    "        types = len(unqie_words)\n",
    "        tokens = len (all_words)\n",
    "        \n",
    "        return types/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193798449612403"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_type_token_ratio(\"text_8_tokenized.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"text_8.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    all_lines = file.readlines()\n",
    "    m = Mystem()\n",
    "    for line in all_lines:\n",
    "        if(line != '\\n' ):\n",
    "            mystemmed_line = m.analyze(line)\n",
    "            print(mystemmed_line)\n",
    "            print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
