{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–,»«'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "full_punctuation = punctuation + \"–\" + \",\" + \"»\" + \"«\"\n",
    "full_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Before: ['Ай да А.С.', 'Пушкин!', 'Ай да сукин сын!']\n",
      "After: ['Ай да А.С. Пушкин!', 'Ай да сукин сын!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('russian.pickle')\n",
    "text = \"Ай да А.С. Пушкин! Ай да сукин сын!\"\n",
    "print(\"Before:\", nltk.sent_tokenize(text))\n",
    "print(\"After:\", tokenizer.tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    clean_sentence = []\n",
    "    #print(sentences)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        clean_text= ''\n",
    "        for word in words:\n",
    "            clean_word = ''\n",
    "            for char in word:\n",
    "                if char != \" \" and char not in full_punctuation:\n",
    "                    clean_word += char.lower()\n",
    "            clean_text += clean_word + ' '\n",
    "        clean_text = re.sub(' +', ' ', clean_text)\n",
    "        clean_text = clean_text.strip()\n",
    "        clean_sentence.append(clean_text)\n",
    "    return clean_sentence\n",
    "\"\"\"    \n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens #if token not in russian_stopwords\n",
    "              if token != \" \" and token.strip() not in full_punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\"\"\"\n",
    "    \n",
    "\n",
    "def clean_and_write_file(file, destination):\n",
    "    preprocessed_text = []\n",
    "    lines = 0\n",
    "    with open (file, \"r\", encoding = \"utf-8\") as file:\n",
    "        all_lines = file.readlines()\n",
    "        bar = progressbar.ProgressBar(maxval=len(all_lines),\n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        bar.start()\n",
    "        for line in all_lines:\n",
    "            t = preprocess_text(line)\n",
    "            #print(t)\n",
    "            preprocessed_text.extend(t)\n",
    "            lines += 1\n",
    "            bar.update(lines)\n",
    "            sleep(0.1)               \n",
    "            \n",
    "    with open (destination, \"w+\", encoding = \"utf-8\") as write_file:\n",
    "        for sentence in preprocessed_text:\n",
    "            #print(\"write\", sentence)\n",
    "            write_file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    }
   ],
   "source": [
    "clean_and_write_file(\"text_8.txt\", \"text_8_processed_tr.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate_lix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_lix(processed_text):\n",
    "      with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "        sentences_count = len(sentences)\n",
    "        words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "        long_words_count = 0 #more than 6\n",
    "        for line in sentences:\n",
    "            for word in line.split():\n",
    "                if len(word) > 6:\n",
    "                    long_words_count += 1\n",
    "        lix = words_count/ sentences_count + (long_words_count * 100) / words_count\n",
    "        print(lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.771679197994985\n"
     ]
    }
   ],
   "source": [
    "calculate_lix(\"text_8_processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_line(text,morph):\n",
    "    \"\"\"\n",
    "    составляем словарь лемматизированных существительных и словарь всех лемм \n",
    "    \"\"\"\n",
    "    lemm_split_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        parsed_word = morph.parse(word)[0]\n",
    "        lemma = parsed_word.normal_form      \n",
    "        lemm_split_text.append(lemma)\n",
    "    return ' '.join(lemm_split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-36370d92bce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmorph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlemmatize_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"составляем словарь лемматизированных существительных\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmorph\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatize_line' is not defined"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmatize_line(\"составляем словарь лемматизированных существительных\",morph )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text_from_file(file, destination):\n",
    "    lemm_text_lines = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    with open(file, \"r\", encoding = \"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            lemm_line = lemmatize_line(line, morph)\n",
    "            lemm_text_lines.append(lemm_line)\n",
    "    with open(destination, \"w\", encoding = \"utf-8\") as d:\n",
    "        for line in lemm_text_lines:\n",
    "            #print(\"write\", sentence)\n",
    "            d.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatize_text_from_file(\"text_8_clean.txt\", \"text_8_lemm_n.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type_token_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_type_token_ratio(lemm_text):\n",
    "      with open (lemm_text, \"r\", encoding = \"utf-8\") as file:\n",
    "          sentences = file.readlines()\n",
    "          pos_list = []\n",
    "          words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "          all_words = []\n",
    "          for sentence in sentences:\n",
    "              words = sentence.split()\n",
    "              for word in words:\n",
    "                  all_words.append(word)\n",
    "                  \n",
    "          unqie_words = set(all_words)\n",
    "          types = len(unqie_words)\n",
    "          tokens = len (all_words)\n",
    "          \n",
    "          return types/tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193798449612403"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_type_token_ratio(\"text_8_lemm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# РАЗНООБРАЗИЕ ЛИЦ -- обсудить необходимость фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'делать', 'wt': 1, 'gr': 'V,несов,пе=прош,мн,изъяв'}], 'text': 'делали'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "print(mystem.analyze(\"делали\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='делать', tag=OpencorporaTag('INFN,impf,tran'), normal_form='делать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'делать', 303, 0),))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"делать\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отношение нефинитных глаголов, инфинитивов, бы, себя, причастия, деепричастия ко всем глаголам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "нефинитных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='делать', tag=OpencorporaTag('INFN,impf,tran'), normal_form='делать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'делать', 303, 0),))]\n",
      "[Parse(word='сделать', tag=OpencorporaTag('INFN,perf,tran'), normal_form='сделать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'сделать', 614, 0),))]\n",
      "[Parse(word='пить', tag=OpencorporaTag('INFN,impf,tran'), normal_form='пить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'пить', 444, 0),))]\n"
     ]
    }
   ],
   "source": [
    "#инфинитивов\n",
    "print(morph.parse(\"делать\"))\n",
    "print(morph.parse(\"сделать\"))\n",
    "print(morph.parse(\"пить\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#бы просто считаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='одеваться', tag=OpencorporaTag('INFN,impf,intr'), normal_form='одеваться', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'одеваться', 224, 0),))]\n",
      "[Parse(word='пастись', tag=OpencorporaTag('INFN,impf,intr'), normal_form='пастись', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'пастись', 2288, 0),))]\n",
      "[Parse(word='светает', tag=OpencorporaTag('VERB,impf,intr,Impe sing,pres,indc'), normal_form='светать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'светает', 658, 2),))]\n"
     ]
    }
   ],
   "source": [
    "#возвратные - себя сь ся\n",
    "print(morph.parse(\"одеваться\"))\n",
    "print(morph.parse(\"пастись\"))\n",
    "print(morph.parse(\"светает\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Количество бы, не, ни (проблема отрицания)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "просто поиск слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в классе был урок русского языка\n",
      "\n",
      "PR=\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "A=(вин\n",
      "A\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "преподаватель читал новый текст\n",
      "\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "A=(вин\n",
      "A\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "студенты внимательно слушали\n",
      "\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "ADV=\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "извините можно войти\n",
      "\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "ADV\n",
      "ADV\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "я опоздал сказал джон\n",
      "\n",
      "SPRO\n",
      "SPRO\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "V\n",
      "V\n",
      "n\n",
      "n\n",
      "S\n",
      "S\n",
      "n\n",
      "n\n",
      "что случилось джон\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f036ee0bc790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmystemmed_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmystem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmystemmed_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmystemmed_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmystemmed_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open (\"text_8_processed.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    \n",
    "    sentences = file.readlines()\n",
    "    words_count = sum([len(line.split(' ')) for line in sentences])\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        mystemmed_line = mystem.analyze(sentence)\n",
    "        for mystemmed_word in mystemmed_line:\n",
    "            keys = list(mystemmed_word.keys())\n",
    "            values = list(mystemmed_word.values())\n",
    "            word  = mystemmed_word['text']\n",
    "            if ('analysis' not in keys):\n",
    "                pass\n",
    "            elif(mystemmed_word['analysis'] ==[]):\n",
    "                pass\n",
    "            else:\n",
    "              grammar = mystemmed_word['analysis'][0]['gr']\n",
    "              grammar_sep_by_comma = grammar.split(',')\n",
    "              print(grammar_sep_by_comma[0])\n",
    "              if (len(grammar_sep_by_comma) == 1):\n",
    "                    \n",
    "                    pass\n",
    "              else:\n",
    "                pos = re.match('[A-Zs]+', grammar_sep_by_comma[0])[0]\n",
    "                #print(pos)\n",
    "           # print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(processed_text, destination):\n",
    "      mystem = Mystem()\n",
    "      with open (processed_text, \"r\", encoding = \"utf-8\") as file:\n",
    "          sentences = file.readlines()\n",
    "          clean_sentences = []\n",
    "          for sentence in sentences:\n",
    "              clean_sentence = ''\n",
    "              mystemmed_line = mystem.analyze(sentence)\n",
    "              for mystemmed_word in mystemmed_line:\n",
    "                  keys = list(mystemmed_word.keys())\n",
    "                  values = list(mystemmed_word.values())\n",
    "                  word  = mystemmed_word['text']\n",
    "                  if ('analysis' not in keys):\n",
    "                      pass\n",
    "                  elif(mystemmed_word['analysis'] ==[]):\n",
    "                      pass\n",
    "                  else:\n",
    "                    grammar = mystemmed_word['analysis'][0]['gr']\n",
    "                    grammar_sep_by_comma = grammar.split(',')\n",
    "                    print(grammar_sep_by_comma[0])\n",
    "                    if (len(grammar_sep_by_comma) == 1):\n",
    "                          pass\n",
    "                    else:\n",
    "                          pos = re.match('[A-Zs]+', grammar_sep_by_comma[0])[0]\n",
    "                          print(pos)\n",
    "\n",
    "                    \n",
    "                  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
